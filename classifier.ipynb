{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vogrCA5aYPtQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk #install using \"conda install anaconda::nltk\"\n",
    "import sklearn \n",
    "import string\n",
    "import re # helps you filter urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQGjuhliYPtV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/27abd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/27abd/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/27abd/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/27abd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/27abd/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download NLTK models\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "lemmatizer= nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords= stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    . and I pledge to be trusted partners who will...\n",
      "1    Great to be with Liza ColÃ³n-Zayas to cook up a...\n",
      "2    Kamala Harris wants to flood the United States...\n",
      "3    Kamala Harris wants to raise taxes on American...\n",
      "4    When President Trump is elected he will usher ...\n",
      "Name: Content, dtype: object \n",
      " 1125\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\") \n",
    "train_tweets = train[\"Content\"]\n",
    "print(train_tweets.head(),'\\n',len(train_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times Square was bejeweled this morning with our new ads ðŸ’Ž\n",
      "['Times', 'Square', 'was', 'bejeweled', 'this', 'morning', 'with', 'our', 'new', 'ads', 'ðŸ’Ž'] \n",
      "\n",
      "Times Square was bejeweled this morning with our new ads \n",
      "['Times', 'Square', 'was', 'bejeweled', 'this', 'morning', 'with', 'our', 'new', 'ads'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on one tweet\n",
    "print(train_tweets[199])\n",
    "\n",
    "#basic tokenization\n",
    "tokens = word_tokenize(train_tweets[199])\n",
    "print(tokens,'\\n')\n",
    "\n",
    "# remove emoji's\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "print(emoji_pattern.sub(r'', train_tweets[199])) # no emoji\n",
    "tokens = word_tokenize(emoji_pattern.sub(r'', train_tweets[199])) \n",
    "print(tokens,'\\n') #no emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‡ºðŸ‡¸ making world's best is what we're proud of doing off-the-shelf! And more proud:https://www.google.com\n",
      "ðŸ‡ºðŸ‡¸ making world's best is what we're proud of doing off-the-shelf! And more proud:\n",
      " making world's best is what we're proud of doing off-the-shelf! And more proud:\n",
      " making world's best is what we're proud of doing off-the-shelf! And more proud:\n",
      " making world best is what we're proud of doing off-the-shelf! And more proud:\n",
      "making world best is what were proud of doing off the shelf  And more proud\n",
      "['making', 'world', 'best', 'is', 'what', 'were', 'proud', 'of', 'doing', 'off', 'the', 'shelf', 'and', 'more', 'proud'] \n",
      "\n",
      "making\n",
      "['making', 'world', 'best', 'is', 'what', 'were', 'proud', 'of', 'doing', 'off', 'the', 'shelf', 'and', 'more', 'proud']\n"
     ]
    }
   ],
   "source": [
    "#tweet processing example\n",
    "example_tweet = \"ðŸ‡ºðŸ‡¸ making world's best is what we're proud of doing off-the-shelf! And more proud:https://www.google.com\"\n",
    "#tempstr = gop_tweets[temp_num]\n",
    "print(example_tweet)\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#handle url\n",
    "print(re.sub(r'http\\S+', '', example_tweet))\n",
    "example_tweet = re.sub(r'http\\S+', '', example_tweet)\n",
    "\n",
    "#handle emojis\n",
    "print(emoji_pattern.sub(r'', example_tweet)) # no emoji\n",
    "example_tweet = emoji_pattern.sub(r'', example_tweet)\n",
    "\n",
    "#handle apostrophe 's\n",
    "print(example_tweet)\n",
    "print(example_tweet.replace(\"'s\",\"\")) #replace 's to \n",
    "example_tweet = example_tweet.replace(\"'s\",\"\")\n",
    "\n",
    "#handle other apostrophe\n",
    "example_tweet = example_tweet.replace(\"'\",\"\")\n",
    "\n",
    "#handle all other punctuation\n",
    "print(re.sub(r'[' + string.punctuation + r']+', ' ', example_tweet).strip())\n",
    "example_tweet = re.sub(r'[' + string.punctuation + r']+', ' ', example_tweet).strip()\n",
    "\n",
    "#handle lowercase\n",
    "example_tweet = example_tweet.lower()\n",
    "\n",
    "tokens = word_tokenize(example_tweet)\n",
    "\n",
    "print(tokens,'\\n')\n",
    "\n",
    "#lemmatize tokens\n",
    "#lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#lemmatize single word or token\n",
    "token_lemma = lemmatizer.lemmatize(tokens[0])\n",
    "print(token_lemma)\n",
    "\n",
    "#lemmatize a list of words or tokens\n",
    "tweet_lemma = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(tweet_lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5IMtsQoYPta"
   },
   "outputs": [],
   "source": [
    "# Convert part of speech tag from nltk.pos_tag to word net compatible format\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    \n",
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    #handle url\n",
    "    processed = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    #handle emojis\n",
    "    processed = emoji_pattern.sub(r'', processed)\n",
    "\n",
    "    #handle apostrophe 'so \n",
    "    processed = processed.replace(\"'s\",\"\")\n",
    "\n",
    "    #handle other apostrophe\n",
    "    processed = processed.replace(\"'\",\"\")\n",
    "\n",
    "    #handle all other punctuation\n",
    "    processed = re.sub(r'[' + string.punctuation + r']+', ' ', processed).strip()\n",
    "\n",
    "    #handle lowercase\n",
    "    processed = processed.lower()\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(processed)\n",
    "\n",
    "    # makes dictionary mapping token to treebank tag\n",
    "    treebank_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # hold lemma tokens\n",
    "    lemma_tokens = []\n",
    "\n",
    "    for word, tree_tag in treebank_tags:\n",
    "        # get corresponding wordnet tag from treebank tag\n",
    "        wordnet_tag = get_wordnet_pos(tree_tag)\n",
    "\n",
    "        # get lemma version of word and append to list\n",
    "        lemma_token = lemmatizer.lemmatize(word, pos=wordnet_tag)\n",
    "        lemma_tokens.append(str(lemma_token))\n",
    "    \n",
    "    return lemma_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrEzaolBYPtf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['im', 'do', 'well', 'how', 'about', 'you']\n",
      "['education', 'be', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'lose', 'your', 'temper', 'or', 'your', 'self', 'confidence']\n",
      "['be', 'have', 'do', 'language', 'city', 'mice']\n",
      "['it', 'hilarious', 'check', 'it', 'out']\n",
      "['see', 'it', 'sunday', 'morning', 'at', '8', '30a', 'on', 'rtv6', 'and', 'our', 'rtv6', 'app']\n",
      "['i', 'love', 'usa', 'have', 'you', 'go', 'to', 'the', 'store', 'yet']\n",
      "['it', 'very', 'chilly', 'outside', 'hector', 'dog', 'will', 'be', 'inside', 'today', 'and', 'for', 'the', 'foreseeable', 'future']\n"
     ]
    }
   ],
   "source": [
    "# Test processing function\n",
    "print(process(\"I'm doing well! How about you?\"))\n",
    "# ['im', 'do', 'well', 'how', 'about', 'you']\n",
    "\n",
    "print(process(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\"))\n",
    "# ['education', 'be', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'lose', 'your', 'temper', 'or', 'your', 'self', 'confidence']\n",
    "\n",
    "print(process(\"been had done languages cities mice\"))\n",
    "# ['be', 'have', 'do', 'language', 'city', 'mice']\n",
    "\n",
    "print(process(\"It's hilarious. Check it out http://t.co/dummyurl\"))\n",
    "# ['it', 'hilarious', 'check', 'it', 'out']\n",
    "\n",
    "print(process(\"See it Sunday morning at 8:30a on RTV6 and our RTV6 app. http:â€¦\"))\n",
    "# ['see', 'it', 'sunday', 'morning', 'at', '8', '30a', 'on', 'rtv6', 'and', 'our', 'rtv6', 'app']\n",
    "\n",
    "print(process(\"I LOVE usA ðŸ‡ºðŸ‡¸!!! Have you gone to the store yet?\"))\n",
    "print(process(\"It's very chilly outside! Hector's dog will be inside today and for the foreseeable futures....\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SM7hJBdOYPtl"
   },
   "outputs": [],
   "source": [
    "def process_all(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" process all text in the dataframe using process() function.\n",
    "    Inputs\n",
    "        df: pd.DataFrame: dataframe containing a column 'Content' loaded from the CSV file\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs\n",
    "        pd.DataFrame: dataframe in which the values of Content column have been changed from str to list(str),\n",
    "                        the output from process() function. Other columns are unaffected.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply_process(text):\n",
    "        if isinstance(text, str):  # Check if the input is a string\n",
    "            lemma_list = process(text)\n",
    "            return lemma_list\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    df['Content'] = df['Content'].apply(apply_process)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXQP6CAiYPto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Content    handle\n",
      "0  [and, i, pledge, to, be, trust, partner, who, ...  Tim_Walz\n",
      "1  [great, to, be, with, liza, colÃ³n, zayas, to, ...  Tim_Walz\n",
      "2  [kamala, harris, want, to, flood, the, united,...   JDVance\n",
      "3  [kamala, harris, want, to, raise, tax, on, ame...   JDVance\n",
      "4  [when, president, trump, be, elect, he, will, ...       GOP\n"
     ]
    }
   ],
   "source": [
    "# Process tweets from training data\n",
    "train = pd.read_csv(\"train.csv\") \n",
    "processed_tweets = process_all(train)\n",
    "\n",
    "print(processed_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NQ9wVm3YPt0"
   },
   "outputs": [],
   "source": [
    "# derive feature vectors\n",
    "def identity(x):\n",
    "    return x\n",
    "def create_features(processed_tweets, stop_words):\n",
    "    \"\"\" creates the feature matrix using the processed tweet text\n",
    "    Inputs:\n",
    "        processed_tweets: pd.DataFrame: processed tweets read from train/test csv file, containing the column 'Content'\n",
    "        stop_words: list(str): stop_words by nltk stopwords (after processing)\n",
    "    Outputs:\n",
    "        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
    "            we need this to tranform test tweets in the same way as train tweets\n",
    "        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
    "    \"\"\"\n",
    "    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False, tokenizer=identity, min_df=2, stop_words=stop_words)\n",
    "    x = vectorizer.fit_transform(processed_tweets)\n",
    "    return (vectorizer, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXYtqEyvYPt4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/cs-418/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/cs-418/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TfidfVectorizer(lowercase=False, min_df=2,\n",
       "                 stop_words=[np.str_('i'), np.str_('me'), np.str_('my'),\n",
       "                             np.str_('myself'), np.str_('we'), np.str_('our'),\n",
       "                             np.str_('ours'), np.str_('ourselves'),\n",
       "                             np.str_('you'), np.str_('youre'), np.str_('youve'),\n",
       "                             np.str_('youll'), np.str_('youd'), np.str_('your'),\n",
       "                             np.str_('yours'), np.str_('yourself'),\n",
       "                             np.str_('yourselves'), np.str_('he'),\n",
       "                             np.str_('him'), np.str_('his'), np.str_('himself'),\n",
       "                             np.str_('she'), np.str_('she'), np.str_('her'),\n",
       "                             np.str_('hers'), np.str_('herself'), np.str_('it'),\n",
       "                             np.str_('it'), np.str_('it'), np.str_('itself'), ...],\n",
       "                 tokenizer=<function identity at 0x169bf14c0>),\n",
       " <1125x1365 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 11830 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_stopwords = list(np.concatenate([process(word) for word in stopwords]))\n",
    "(tfidf, X) = create_features(processed_tweets[\"Content\"],processed_stopwords)\n",
    "\n",
    "tfidf, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLtDPaInYPt7"
   },
   "outputs": [],
   "source": [
    "def create_labels(processed_tweets):\n",
    "    \"\"\" creates the class labels from handle\n",
    "    Inputs:\n",
    "        processed_tweets: pd.DataFrame: tweets read from train file, containing the column 'handle'\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary numpy array of class labels\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for index, row in processed_tweets.iterrows():\n",
    "        if row['handle'] in ['realDonaldTrump', 'JDVance', 'GOP']:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z49a4djKYPt-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "y = create_labels(processed_tweets)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xUBpZ6_NYPuB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5377777777777778\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class MajorityLabelClassifier():\n",
    "    \"\"\"\n",
    "    A classifier that predicts the mode of training labels\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize your parameter here\n",
    "        \"\"\"\n",
    "        self.mode = -1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Implement fit by taking training data X and their labels y and finding the mode of y\n",
    "        i.e. store your learned parameter\n",
    "        \"\"\"\n",
    "        count_0 = y.count(0)\n",
    "        count_1 = y.count(1)\n",
    "\n",
    "        if count_0 > count_1:\n",
    "            self.mode = 0\n",
    "        else:\n",
    "            self.mode = 1\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Implement to give the mode of training labels as a prediction for each data instance in X\n",
    "        return labels\n",
    "        \"\"\"\n",
    "        return [self.mode] * X.shape[0]\n",
    "\n",
    "# Get accuracy of classifier by comparing the predicted label of each example to its true label\n",
    "baselineClf = MajorityLabelClassifier()\n",
    "\n",
    "# Use fit and predict methods to get predictions and compare it with the true labels y\n",
    "baselineClf.fit(X,y)\n",
    "predictions = baselineClf.predict(X)\n",
    "\n",
    "# Evaluate the classifier (e.g., using accuracy)\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_accuracy = accuracy_score(y, predictions)\n",
    "print(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGpfOK2EYPuD"
   },
   "outputs": [],
   "source": [
    "def learn_classifier(X_train, y_train, penalty):\n",
    "    \"\"\" learns a classifier from the input features and labels using the penalty function supplied\n",
    "    Inputs:\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features, output of create_features()\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels, output of create_labels()\n",
    "        penalty: str: penalty function to be used with classifier. [none|l2|l1|elasticnet]\n",
    "    Outputs:\n",
    "        sklearn.linear_model.LogisticRegression: classifier learnt from data\n",
    "    \"\"\"\n",
    "    model = None\n",
    "    if penalty != 'elasticnet':\n",
    "        model = sklearn.linear_model.LogisticRegression(penalty=penalty, solver='saga')\n",
    "    else:\n",
    "        model = sklearn.linear_model.LogisticRegression(penalty=penalty, solver='saga', l1_ratio=0.5)\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOwIKVuvYPuF"
   },
   "outputs": [],
   "source": [
    "classifier = learn_classifier(X, y, 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvLsjxqKYPuL"
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, X_validation, y_validation):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "    Inputs:\n",
    "        classifier: sklearn.linear_model.LogisticRegression: classifer to evaluate\n",
    "        X_validation: scipy.sparse.csr.csr_matrix: sparse matrix of features\n",
    "        y_validation: numpy.ndarray(int): dense binary vector of class labels\n",
    "    Outputs:\n",
    "        double: accuracy of classifier on the validation data\n",
    "    \"\"\"\n",
    "    prediction = classifier.predict(X_validation)\n",
    "\n",
    "    return sklearn.metrics.accuracy_score(y_validation, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pmdMkFIzYPuN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9271111111111111\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_classifier(classifier, X, y)\n",
    "print(accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nq5AlMoUYPuQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=4, random_state=1, shuffle=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = sklearn.model_selection.KFold(n_splits=4, random_state=1, shuffle=True)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJAjQyNKYPuT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y type: <class 'list'>\n",
      "None accuracy: 0.8186788571717019\n",
      "l2 accuracy: 0.8320177431160243\n",
      "l1 accuracy: 0.7582847479871786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/cs-418/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/cs-418/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/cs-418/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/cs-418/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticnet accuracy: 0.8071319502284142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'l2'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def best_model_selection(kf, X, y):\n",
    "    \"\"\"\n",
    "    Select the penalty giving best results using k-fold cross-validation.\n",
    "    Other parameters should be left default.\n",
    "    Input:\n",
    "    kf (sklearn.model_selection.KFold): kf object defined above\n",
    "    X (scipy.sparse.csr.csr_matrix): training data\n",
    "    y (array(int)): training labels\n",
    "    Return:\n",
    "    best_penalty (string)\n",
    "    \"\"\"\n",
    "\n",
    "    # indices = np.array([0, 2])\n",
    "    # print(\"X\", X[indices, :])\n",
    "    best_model = ''\n",
    "    best_accuracy = -1\n",
    "    print(\"y type:\", type(y))\n",
    "    for penalty in [None, 'l2', 'l1', 'elasticnet']:\n",
    "        total_accuracy = 0\n",
    "         # Use the documentation of KFold cross-validation to split ..\n",
    "          # training data and test data from create_features() and create_labels()\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index, :], X[test_index, :]\n",
    "\n",
    "            y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "\n",
    "            # call learn_classifer() using training split of kth fold\n",
    "            classifier = learn_classifier(X_train, y_train, penalty)\n",
    "\n",
    "            # evaluate on the test split of kth fold\n",
    "            accuracy = evaluate_classifier(classifier, X_test, y_test)\n",
    "            total_accuracy += accuracy\n",
    "\n",
    "        # record avg accuracies and determine best model (penalty)\n",
    "        avg_accuracy = total_accuracy / kf.get_n_splits()\n",
    "        print(penalty, \"accuracy:\", avg_accuracy)\n",
    "        if avg_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_accuracy\n",
    "            best_model = penalty    \n",
    "\n",
    "        \n",
    "    #return best penalty as string\n",
    "    return best_model\n",
    "\n",
    "best_penalty = best_model_selection(kf, X, y)\n",
    "best_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qth7DPekYPuY"
   },
   "outputs": [],
   "source": [
    "def classify_tweets(tfidf, classifier, unlabeled_tweets):\n",
    "    \"\"\" predicts class labels for raw tweet text\n",
    "    Inputs:\n",
    "        tfidf: sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used on training data\n",
    "        classifier: sklearn.linear_model.LogisticRegression: classifier learned\n",
    "        unlabeled_tweets: pd.DataFrame: tweets read from tweets_test.csv\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary vector of class labels for unlabeled tweets\n",
    "    \"\"\"\n",
    "    processed_unlabeled = process_all(unlabeled_tweets)\n",
    "\n",
    "    X = tfidf.transform(processed_unlabeled['Content'])\n",
    "\n",
    "    return classifier.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKNMwVVgYPua"
   },
   "outputs": [],
   "source": [
    "# Get predictions for unlabelled test data\n",
    "classifier = learn_classifier(X, y, best_penalty)\n",
    "unlabeled_tweets = pd.read_csv(\"test.csv\", na_filter=False)\n",
    "y_pred = classify_tweets(tfidf, classifier, unlabeled_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oeuZhVR2OhyV"
   },
   "source": [
    "*The Logistic Regression classifier did better than the baseline using each penalty. The baseline had an accuracy of 53%, but my classifier when using L2 was 83% which is significantly better.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i, think, he, know, who, you, be]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[will, the, network, that, obsessively, cover,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[i, will, not, raise, tax, on, anyone, make, l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[democrat, republican, and, independent, be, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[over, the, next, 9, day, let, â€™, s, be, inten...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>[the, american, people, be, exhaust, with, the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>[thank, you, for, your, support, mr, president...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>[great, to, be, in, wilmington, with, north, c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>[kamala, harris, just, say, go, to, her, websi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>[usa, usa, usa]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Content  Prediction\n",
       "0                   [i, think, he, know, who, you, be]           1\n",
       "1    [will, the, network, that, obsessively, cover,...           0\n",
       "2    [i, will, not, raise, tax, on, anyone, make, l...           0\n",
       "3    [democrat, republican, and, independent, be, s...           1\n",
       "4    [over, the, next, 9, day, let, â€™, s, be, inten...           1\n",
       "..                                                 ...         ...\n",
       "194  [the, american, people, be, exhaust, with, the...           1\n",
       "195  [thank, you, for, your, support, mr, president...           1\n",
       "196  [great, to, be, in, wilmington, with, north, c...           1\n",
       "197  [kamala, harris, just, say, go, to, her, websi...           0\n",
       "198                                    [usa, usa, usa]           0\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_tweets['Prediction'] = y_pred\n",
    "\n",
    "unlabeled_tweets"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
